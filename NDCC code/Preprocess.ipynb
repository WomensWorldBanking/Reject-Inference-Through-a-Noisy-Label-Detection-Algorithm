{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7980f635-ed3c-4b76-a572-7e7e4ff24f53",
   "metadata": {},
   "source": [
    "### OCI Data Science - Useful Tips\n",
    "<details>\n",
    "<summary><font size=\"2\">Check for Public Internet Access</font></summary>\n",
    "\n",
    "```python\n",
    "import requests\n",
    "response = requests.get(\"https://oracle.com\")\n",
    "assert response.status_code==200, \"Internet connection failed\"\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Helpful Documentation </font></summary>\n",
    "<ul><li><a href=\"https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm\">Data Science Service Documentation</a></li>\n",
    "<li><a href=\"https://docs.cloud.oracle.com/iaas/tools/ads-sdk/latest/index.html\">ADS documentation</a></li>\n",
    "</ul>\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Typical Cell Imports and Settings for ADS</font></summary>\n",
    "\n",
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "\n",
    "import ads\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from ads.automl.provider import OracleAutoMLProvider\n",
    "from ads.automl.driver import AutoML\n",
    "from ads.evaluations.evaluator import ADSEvaluator\n",
    "from ads.common.data import ADSData\n",
    "from ads.explanations.explainer import ADSExplainer\n",
    "from ads.explanations.mlx_global_explainer import MLXGlobalExplainer\n",
    "from ads.explanations.mlx_local_explainer import MLXLocalExplainer\n",
    "from ads.catalog.model import ModelCatalog\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Useful Environment Variables</font></summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "print(os.environ[\"NB_SESSION_COMPARTMENT_OCID\"])\n",
    "print(os.environ[\"PROJECT_OCID\"])\n",
    "print(os.environ[\"USER_OCID\"])\n",
    "print(os.environ[\"TENANCY_OCID\"])\n",
    "print(os.environ[\"NB_REGION\"])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be2f0c01-9328-47e5-a04a-d958c45fec38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/datascience/conda/pytorch20_p39_gpu_v2/lib/python3.9/site-packages (1.12.1)\n",
      "Requirement already satisfied: torchvision in /home/datascience/conda/pytorch20_p39_gpu_v2/lib/python3.9/site-packages (0.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/datascience/conda/pytorch20_p39_gpu_v2/lib/python3.9/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: numpy in /home/datascience/conda/pytorch20_p39_gpu_v2/lib/python3.9/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: requests in /home/datascience/conda/pytorch20_p39_gpu_v2/lib/python3.9/site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/datascience/conda/pytorch20_p39_gpu_v2/lib/python3.9/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/datascience/conda/pytorch20_p39_gpu_v2/lib/python3.9/site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/datascience/conda/pytorch20_p39_gpu_v2/lib/python3.9/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/datascience/conda/pytorch20_p39_gpu_v2/lib/python3.9/site-packages (from requests->torchvision) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/datascience/conda/pytorch20_p39_gpu_v2/lib/python3.9/site-packages (from requests->torchvision) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99249778-9cba-4c97-b98f-4415ffaccb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from torch.utils.data import TensorDataset\n",
    "import os\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d3d6c01-b958-4c54-bc20-b3fe9cfd58d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of changed labels: 1170\n",
      "Data saved to D_noisy.pth: Features torch.Size([5516, 70]), Labels torch.Size([5516])\n",
      "Data saved to D_clean.pth: Features torch.Size([5516, 70]), Labels torch.Size([5516])\n",
      "Changed indices saved to D_clean_changed_indices.npy\n",
      "Data preprocessing complete. Tensors saved.\n"
     ]
    }
   ],
   "source": [
    "# This part of the code loads the data, preprocesses it, and creates a clean version of the data based on our defined rules.\n",
    "# The first rule flips labels from 1 to 0 for those data instances where the loan is not paid off and the ratio of number_of_missed_payments to number_of_repayments is greater than or equal to 2.\n",
    "# The second rule flips labels from 0 to 1 for those data instances where the CreditScore is greater than or equal to 4, DefaultedLoansWithin90Days is 0, and last_defaults_12_months is 0.\n",
    "\n",
    "columns_to_remove = [\n",
    "    'disbursedon_date', 'principal_amount', 'Repayment_Period',\n",
    "    'number_of_repayments', 'principal_outstanding', 'total_expected_repayment',\n",
    "    'total_outstanding', 'loan_number', 'missed_payments', 'defaults_12_months', 'pay_off'\n",
    "]\n",
    "categorical_cols = ['Gender', 'state', 'HasDeliquentCreditInHistory', 'last_HasDeliquentCreditInHistory', 'last_pay_off']\n",
    "\n",
    "def load_data(csv_file_path):\n",
    "    return pd.read_csv(csv_file_path)\n",
    "\n",
    "def preprocess_data(df, categorical_cols):\n",
    "    full_features = df.drop(['decision'] + columns_to_remove, errors='ignore', axis=1)\n",
    "    full_features = pd.get_dummies(full_features, columns=categorical_cols, drop_first=True)\n",
    "    dummy_column_names = full_features.columns.tolist()\n",
    "    scaler = StandardScaler()\n",
    "    numeric_cols = full_features.select_dtypes(include=['number']).columns.tolist()\n",
    "    full_features[numeric_cols] = scaler.fit_transform(full_features[numeric_cols])\n",
    "    return full_features, df['decision'], dummy_column_names\n",
    "\n",
    "def save_data(features, labels, filename, changed_indices=None):\n",
    "    features_tensor = torch.tensor(features.values, dtype=torch.float32)\n",
    "    labels_tensor = torch.tensor(labels.values, dtype=torch.long)\n",
    "    dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "    torch.save(dataset, filename)\n",
    "    print(f\"Data saved to {filename}: Features {features_tensor.shape}, Labels {labels_tensor.shape}\")\n",
    "    if changed_indices is not None:\n",
    "        np.save(filename.replace('.pth', '_changed_indices.npy'), changed_indices)\n",
    "        print(f\"Changed indices saved to {filename.replace('.pth', '_changed_indices.npy')}\")\n",
    "\n",
    "filename = 'Existing customer data with missing value handling using clustering-Apr18.csv'\n",
    "df = load_data(filename)\n",
    "condition1_indices = df[(df['decision'] == 1) & (df['pay_off'] == -1) & ((df['missed_payments'] / df['number_of_repayments']) >= 2)].index\n",
    "condition2_indices = df[(df['decision'] == 0) & (df['CreditScore'] >= 4) & (df['DefaultedLoansWithin90Days'] == 0) & (df['last_defaults_12_months'] == 0)].index\n",
    "\n",
    "df_clean = df.copy()\n",
    "changed_indices = condition1_indices.union(condition2_indices)\n",
    "df_clean.loc[changed_indices, 'decision'] = 1 - df_clean.loc[changed_indices, 'decision']\n",
    "changed_labels_count = (df['decision'] != df_clean['decision']).sum()\n",
    "print(f\"Number of changed labels: {changed_labels_count}\")\n",
    "features, labels_noisy, dummy_column_names = preprocess_data(df, categorical_cols)\n",
    "features_clean, labels_clean, _ = preprocess_data(df_clean, categorical_cols)\n",
    "save_data(features, labels_noisy, 'D_noisy.pth')\n",
    "save_data(features_clean, labels_clean, 'D_clean.pth', changed_indices=np.array(list(changed_indices)))\n",
    "\n",
    "print(\"Data preprocessing complete. Tensors saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfea170d-0618-4460-a896-296c70d6725d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices saved to D_validation_indices.csv\n",
      "Data saved to D_validation_noisy.pth and D_validation_noisy.csv\n",
      "Data saved to D_validation_clean.pth and D_validation_clean.csv\n",
      "Indices saved to D_train_indices.csv\n",
      "Data saved to D_train_noisy.pth and D_train_noisy.csv\n",
      "Data saved to D_train_clean.pth and D_train_clean.csv\n",
      "Data saved to Dpre_first_noisy.pth and Dpre_first_noisy.csv\n",
      "Data saved to Dpre_clean.pth and Dpre_clean.csv\n",
      "Data processing and saving complete.\n"
     ]
    }
   ],
   "source": [
    "# This section of the code splits the main clean and noisy datasets to create the required subsets of data. It creates D_train (used for the primary NDCC algorithm) and D_pre (used for pretraining a model and selecting thresholds) and D_validation for validating the accuracy of the trained model.\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    dataset = torch.load(file_path)\n",
    "    features = dataset.tensors[0].numpy()\n",
    "    labels = dataset.tensors[1].numpy()\n",
    "    df = pd.DataFrame(features, columns=[f'feature_{i}' for i in range(features.shape[1])])\n",
    "    df['decision'] = labels\n",
    "    return df\n",
    "\n",
    "def select_indices(df, label_0_count, label_1_count, name):\n",
    "    df_shuffled = shuffle(df, random_state=42)\n",
    "    decision_0 = df_shuffled[df_shuffled['decision'] == 0]\n",
    "    decision_1 = df_shuffled[df_shuffled['decision'] == 1]\n",
    "    if len(decision_0) < label_0_count or len(decision_1) < label_1_count:\n",
    "        raise ValueError(f\"Not enough data points in one or both decision categories for {name}.\")\n",
    "    indices_0 = decision_0.sample(n=label_0_count, random_state=42).index\n",
    "    indices_1 = decision_1.sample(n=label_1_count, random_state=42).index\n",
    "    selected_indices = indices_0.union(indices_1)\n",
    "    # Save indices\n",
    "    selected_indices.to_series().reset_index(drop=True).to_csv(f'{name}_indices.csv', index=False)\n",
    "    print(f\"Indices saved to {name}_indices.csv\")\n",
    "    return selected_indices\n",
    "\n",
    "def create_subset(df_noisy, df_clean, indices, noisy_dataset_name, clean_dataset_name):\n",
    "    subset_noisy = df_noisy.loc[indices]\n",
    "    subset_clean = df_clean.loc[indices]\n",
    "    save_data(subset_noisy, f'{noisy_dataset_name}.pth')\n",
    "    save_data(subset_clean, f'{clean_dataset_name}.pth')\n",
    "\n",
    "def save_data(df, filename):\n",
    "    features = df.iloc[:, :-1].values  # Assuming last column is 'decision'\n",
    "    labels = df['decision'].values\n",
    "    dataset = TensorDataset(torch.tensor(features, dtype=torch.float32), torch.tensor(labels, dtype=torch.long))\n",
    "    torch.save(dataset, filename)\n",
    "    df.to_csv(filename.replace('.pth', '.csv'), index=False)\n",
    "    print(f\"Data saved to {filename} and {filename.replace('.pth', '.csv')}\")\n",
    "\n",
    "df_noisy = load_dataset('D_noisy.pth')\n",
    "df_clean = load_dataset('D_clean.pth')\n",
    "validation_indices = select_indices(df_noisy, 184, 698, 'D_validation')\n",
    "create_subset(df_noisy, df_clean, validation_indices, 'D_validation_noisy', 'D_validation_clean')\n",
    "\n",
    "df_noisy = df_noisy.drop(validation_indices)\n",
    "df_clean = df_clean.drop(validation_indices)\n",
    "\n",
    "train_indices = select_indices(df_noisy, 739, 2795, 'D_train')\n",
    "create_subset(df_noisy, df_clean, train_indices, 'D_train_noisy', 'D_train_clean')\n",
    "\n",
    "df_noisy = df_noisy.drop(train_indices)\n",
    "df_clean = df_clean.drop(train_indices)\n",
    "\n",
    "pre_indices = df_noisy.index\n",
    "pre_indices.to_series().reset_index(drop=True).to_csv('Dpre_first_indices.csv', index=False)\n",
    "create_subset(df_noisy, df_clean, pre_indices, 'Dpre_first_noisy', 'Dpre_clean')\n",
    "\n",
    "print(\"Data processing and saving complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "569c2c65-b1b8-4416-a4a0-cf4fecb06f09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to Dpre_noisy.pth and Dpre_noisy.csv\n",
      "Flipped indices saved to Dpre_flipped_indices.csv\n"
     ]
    }
   ],
   "source": [
    "# This section of the code restores 10% of the labels changed during data cleaning to create a noisy version of D_pre, which requires a noise level of 10%.\n",
    "\n",
    "def load_dataset(filename):\n",
    "    dataset = torch.load(filename)\n",
    "    features = dataset.tensors[0].numpy()\n",
    "    labels = dataset.tensors[1].numpy()\n",
    "    return pd.DataFrame(features), pd.Series(labels)\n",
    "\n",
    "def save_data(df, labels, filename):\n",
    "    dataset = TensorDataset(torch.tensor(df.values, dtype=torch.float32), torch.tensor(labels.values, dtype=torch.long))\n",
    "    torch.save(dataset, filename)\n",
    "    combined = df.copy()\n",
    "    combined['decision'] = labels\n",
    "    combined = shuffle(combined)  # Shuffle to avoid grouping by label\n",
    "    combined.to_csv(filename.replace('.pth', '.csv'), index=False)\n",
    "    print(f\"Data saved to {filename} and {filename.replace('.pth', '.csv')}\")\n",
    "\n",
    "df_clean_features, labels_clean = load_dataset('Dpre_clean.pth')\n",
    "df_noisy_features, labels_noisy = load_dataset('Dpre_first_noisy.pth')\n",
    "differing_indices = labels_clean[labels_clean != labels_noisy].index\n",
    "\n",
    "num_to_flip = int(0.1 * len(labels_clean))\n",
    "indices_to_flip = np.random.choice(differing_indices, size=num_to_flip, replace=False)\n",
    "\n",
    "final_labels = labels_clean.copy()\n",
    "final_labels.iloc[indices_to_flip] = 1 - final_labels.iloc[indices_to_flip]\n",
    "\n",
    "save_data(df_clean_features, final_labels, 'Dpre_noisy.pth')\n",
    "\n",
    "pd.Series(indices_to_flip).to_csv('Dpre_flipped_indices.csv', index=False)\n",
    "print(f\"Flipped indices saved to Dpre_flipped_indices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "471ae9f9-cd0a-4146-ac28-752965313718",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates csv version of the main noisy dataset.\n",
    "\n",
    "dataset = torch.load('D_noisy.pth')\n",
    "all_features = []\n",
    "all_labels = [] \n",
    "\n",
    "for data_tensor, label_tensor in dataset:\n",
    "    all_features.append(data_tensor.numpy())\n",
    "    all_labels.append(label_tensor.numpy())  \n",
    "\n",
    "features_array = np.vstack(all_features) \n",
    "labels_array = np.vstack(all_labels)  \n",
    "df_features = pd.DataFrame(features_array)\n",
    "df_features.to_csv('D_noisy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c32b2f11-fd97-4794-a246-a94eb41ddeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2150/1801849231.py:7: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  df.fillna(df.mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# This part of the code creates a validation dataset containing the original features.\n",
    "\n",
    "df = pd.read_csv('Existing customer data with original credit score.csv')\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "indices_df = pd.read_csv('D_validation_indices.csv', header=None)\n",
    "validation_indices = indices_df[0].tolist()\n",
    "\n",
    "validation_data = df.iloc[validation_indices]\n",
    "validation_data.to_csv('D_validation_original.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7e65ca-20ce-4e53-86a6-5730363b52fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch20_p39_gpu_v2]",
   "language": "python",
   "name": "conda-env-pytorch20_p39_gpu_v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
